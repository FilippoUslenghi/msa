{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zQ9VAcGjiZ2",
    "outputId": "599ab0e9-f802-4ecd-b93b-d36747013da5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 81 kB 5.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "typer 0.4.2 requires click<9.0.0,>=7.1.1, but you have click 7.0 which is incompatible.\n",
      "earthengine-api 0.1.316 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.7.10 which is incompatible.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install -q --upgrade gupload\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from google.colab import auth\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-myuS9Q_hQts",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWKFMqjfle6h",
    "outputId": "4890998b-838f-48ae-fef8-975e4ba1f10a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Mount the remote storage with the dataset\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\nos.system(\"unzip -q drive/MyDrive/msa/CatsDogs.zip\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Mount the remote storage with the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.system(\"unzip -q drive/MyDrive/msa/CatsDogs.zip\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW3IDNEWhQtt",
    "outputId": "590e2ee0-3f68-4008-c064-c8f111ea51c1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in the dataset: 24997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: CatsDogs/Cats/666.jpg: No such file or directory\n",
      "rm: CatsDogs/Dogs/11702.jpg: No such file or directory\n",
      "rm: CatsDogs/Dogs/11410.jpg: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Define the directory of the dataset\n",
    "data_dir = pathlib.Path('CatsDogs/')\n",
    "\n",
    "# Remove corrupted files\n",
    "os.system(\"rm CatsDogs/Cats/666.jpg CatsDogs/Dogs/11702.jpg CatsDogs/Dogs/11410.jpg\")\n",
    "\n",
    "# Collects the path of all the files within the dataset\n",
    "data_paths = [str(path) for path in list(data_dir.glob(\"*/*.jpg\"))]\n",
    "print(f\"Images in the dataset: {len(data_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwrAqBjehQtu",
    "outputId": "eaa67e7d-8551-49a8-e1f9-f9d7dd2405b3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/filippouslenghi/miniconda3/envs/audio2/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:822: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    }
   ],
   "source": [
    "# Convert non-jpeg images into jpeg files\n",
    "formats = [(path, Image.open(path).format) for path in data_paths]\n",
    "non_jpegs = list(filter(lambda x: x[1]!='JPEG', formats))\n",
    "for path, _ in non_jpegs:\n",
    "    img = Image.open(path)\n",
    "    img.convert('RGB').save(path, format='JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PrZiRi9XhQtt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_label(file_path):\n",
    "    # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    one_hot = parts[-2] == ['Cats','Dogs']\n",
    "    # Integer encode the label\n",
    "    return tf.argmax(one_hot)\n",
    "\n",
    "\n",
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # Resize the image to the desired size\n",
    "    return tf.image.resize(img, [150, 150])\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # Load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "# Configure dataset for performance\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def prepare_data(data, train_index, test_index):\n",
    "\n",
    "    # Get the paths to the data\n",
    "    train_paths = np.asarray(data)[train_index]\n",
    "    test_paths = np.asarray(data)[test_index]\n",
    "\n",
    "    # Make it tf.data.Dataset\n",
    "    train = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "    test = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "\n",
    "    # Get labels\n",
    "    train = train.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test = test.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Configure for performance\n",
    "    train = configure_for_performance(train)\n",
    "    test = configure_for_performance(test)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def subsampling(dataset):\n",
    "    random.shuffle(dataset)\n",
    "    subsampled_dataset = random.sample(dataset, round(len(dataset)/2))\n",
    "    return subsampled_dataset\n",
    "\n",
    "\n",
    "def zero_one_loss(dataset, verbose=0):\n",
    "    missmatches = 0\n",
    "    for i, (data, labels) in enumerate(dataset.as_numpy_iterator()):\n",
    "\n",
    "        y_pred_binary = np.around(model.predict(x=np.asarray(data), verbose=verbose)).flatten()\n",
    "        missmatches += np.sum(np.logical_xor(y_pred_binary, labels))\n",
    "\n",
    "    return round(missmatches, ndigits=0)\n",
    "    \n",
    "def save_results(results):\n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6AvKY7BAhQtu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper-paramters\n",
    "batch_size = 64\n",
    "\n",
    "# Hypter-parameters to by tuned\n",
    "filters_coeffs = ['same', 'incremental']\n",
    "list_n_filters = [16, 32, 64]\n",
    "kernel_sizes = [3, 5, 7]\n",
    "list_n_epochs = [10, 15, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run nested cross validation to find the best hyper-parameters for this architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UIaINbRToRn",
    "outputId": "75bb05d4-b298-4fe7-c73a-1c0900378a86"
   },
   "outputs": [],
   "source": [
    "# Nested cross-val\n",
    "subsampled_data_paths = subsampling(data_paths)\n",
    "k_fold = KFold(n_splits=5)\n",
    "k_splits = list(k_fold.split(subsampled_data_paths))\n",
    "\n",
    "best_model = {}\n",
    "external_results = []\n",
    "internal_count=0\n",
    "\n",
    "train_index, _ = k_splits[0]\n",
    "train_paths = np.asarray(subsampled_data_paths)[train_index]  # Compute training part\n",
    "\n",
    "# Internal cross validation\n",
    "for filters_coeff in filters_coeffs:\n",
    "    for n_filters in list_n_filters:\n",
    "        for kernel_size in kernel_sizes:\n",
    "            for n_epochs in list_n_epochs:\n",
    "                internal_splits = k_fold.split(train_paths)\n",
    "                tmp_results = []\n",
    "                \n",
    "                \n",
    "                if internal_count<21: \n",
    "                    internal_count+=1\n",
    "                    continue\n",
    "                for internal_train_index, internal_test_index in internal_splits:  # Interal cross validation\n",
    "                    train, test = prepare_data(train_paths, internal_train_index, internal_test_index)\n",
    "\n",
    "                    model = tf.keras.Sequential([\n",
    "                        tf.keras.layers.Rescaling(1./255),\n",
    "                        tf.keras.layers.Conv2D(n_filters, kernel_size, activation=tf.nn.relu),\n",
    "                        tf.keras.layers.MaxPooling2D(),\n",
    "                        tf.keras.layers.Conv2D(n_filters * (1, 2)[filters_coeff=='incremental'], kernel_size, activation=tf.nn.relu),\n",
    "                        tf.keras.layers.MaxPooling2D(),\n",
    "                        tf.keras.layers.Conv2D(n_filters * (1, 4)[filters_coeff=='incremental'], kernel_size, activation=tf.nn.relu),\n",
    "                        tf.keras.layers.MaxPooling2D(),\n",
    "                        tf.keras.layers.Flatten(),\n",
    "                        tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "                        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                    ])\n",
    "\n",
    "                    model.compile(\n",
    "                        optimizer='adam',\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy']\n",
    "                    )\n",
    "\n",
    "                    history = model.fit(\n",
    "                        train,\n",
    "                        epochs=n_epochs,\n",
    "                        verbose=0\n",
    "                    )\n",
    "\n",
    "                    internal_loss = zero_one_loss(test)\n",
    "                    tmp_results.append(internal_loss)\n",
    "                    \n",
    "                    # Clear the model\n",
    "                    del model\n",
    "                    tf.keras.backend.clear_session()\n",
    "\n",
    "                result = {'filters_coeff': filters_coeff, \n",
    "                          'n_filters': n_filters, \n",
    "                          'kernel_size': kernel_size,\n",
    "                          'n_epochs': n_epochs,\n",
    "                          'zero_one_loss': np.round(np.mean(tmp_results), decimals=0)}  # Compute the mean loss of the internal cv\n",
    "\n",
    "                internal_results.append(result)\n",
    "                save_results(internal_results)\n",
    "                print(f\"Finished internal iteration {internal_count}\")\n",
    "                internal_count+=1\n",
    "\n",
    "best_model = min(internal_results, key=lambda x: x['zero_one_loss'])\n",
    "print(f'Best hyper parameters: {best_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the external cross validation of the best predictor found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = SimpleNamespace(**best_model)\n",
    "for train_index, test_index in k_splits:\n",
    "    train, test = prepare_data(subsampled_data_paths, train_index, test_index)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.Conv2D(hyper_parameters.n_filters, hyper_parameters.kernel_size, activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(hyper_parameters.n_filters * (1, 2)[hyper_parameters.filters_coeff=='incremental'], hyper_parameters.kernel_size, activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(hyper_parameters.n_filters * (1, 4)[hyper_parameters.filters_coeff=='incremental'], hyper_parameters.kernel_size, activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        epochs=hyper_parameters.n_epochs,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    external_loss = zero_one_loss(test, verbose=1)\n",
    "    external_results.append(external_loss)\n",
    "\n",
    "    # Clear the model\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "mean_zero_one_loss = np.round(np.mean(external_results), decimals=0)\n",
    "std_zero_one_loss = np.round(np.std(external_results), decimals=0)\n",
    "print(f'Zero one loss of best model: {int(mean_zero_one_loss)}, with std of {int(std_zero_one_loss)} samples.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "my_nested_cv.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8 (audio2)",
   "language": "python",
   "name": "audio2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
