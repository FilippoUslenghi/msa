{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 11469,
     "status": "ok",
     "timestamp": 1657212338604,
     "user": {
      "displayName": "Filippo Uslenghi",
      "userId": "02060675236977838476"
     },
     "user_tz": -120
    },
    "id": "Z0mIezofgJgk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('unzip -oq CatsDogs.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1657211842788,
     "user": {
      "displayName": "Filippo Uslenghi",
      "userId": "02060675236977838476"
     },
     "user_tz": -120
    },
    "id": "tORwxj72lLGt",
    "outputId": "e18916b4-e12a-4b9d-d567-e2469d1a3b82",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in the dataset: 24997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: CatsDogs/Cats/666.jpg: No such file or directory\n",
      "rm: CatsDogs/Dogs/11702.jpg: No such file or directory\n",
      "rm: CatsDogs/Dogs/11410.jpg: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Define the directory of the dataset\n",
    "data_dir = pathlib.Path('CatsDogs/')\n",
    "\n",
    "# Remove corrupted files\n",
    "os.system(\"rm CatsDogs/Cats/666.jpg CatsDogs/Dogs/11702.jpg CatsDogs/Dogs/11410.jpg\")\n",
    "\n",
    "# Collects the path of all the files within the dataset\n",
    "data_paths = [str(path) for path in list(data_dir.glob(\"*/*.jpg\"))]\n",
    "print(f\"Images in the dataset: {len(data_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert non-jpeg images into jpeg files\n",
    "# formats = [(path, Image.open(path).format) for path in data_paths]\n",
    "# non_jpegs = list(filter(lambda x: x[1]!='JPEG', formats))\n",
    "# for path, _ in non_jpegs:\n",
    "#    img = Image.open(path)\n",
    "#    img.convert('RGB').save(path, format='JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "orirMLaP754O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 18:55:02.410202: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-19 18:55:02.410332: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Create the respective tf.data.Dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_paths)\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(len(data_paths), reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1657211850149,
     "user": {
      "displayName": "Filippo Uslenghi",
      "userId": "02060675236977838476"
     },
     "user_tz": -120
    },
    "id": "UImc8RD4Ejlh",
    "outputId": "07af990f-15e4-40a9-e5ac-a0f77611f792",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cats' 'Dogs']\n"
     ]
    }
   ],
   "source": [
    "# Get the class names\n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.name[0] != '.']))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5cJWtxfo_K-U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spilt the dataset\n",
    "test_size = int(len(list(dataset)) * 0.2)\n",
    "train = dataset.skip(test_size)\n",
    "test = dataset.take(test_size)\n",
    "\n",
    "# Create a validation set\n",
    "val_size = int(len(list(train))*0.2)\n",
    "val = train.take(val_size)\n",
    "train = train.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VXF62vBlJj9e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set initial params for the loader\n",
    "batch_size = 64\n",
    "img_height = 150\n",
    "img_width = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "arSQzIey-4D4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    one_hot = parts[-2] == class_names\n",
    "    # Integer encode the label\n",
    "    return tf.argmax(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MGlq4IP4Aktb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # Resize the image to the desired size\n",
    "    return tf.image.resize(img, [img_height, img_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-xhBRgvNqRRe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # Load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XsGQx56YHoe9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataset of image, label pairs\n",
    "train = train.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test = test.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val = val.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset for performance\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S3uDN1TiKOFR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = configure_for_performance(train)\n",
    "test = configure_for_performance(test)\n",
    "val = configure_for_performance(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "En29RHVsKWIW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    # tf.keras.layers.Dense(128, activation='leaky_relu'),\n",
    "    # tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(16, 3, activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KM1r5OcXhBhE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 181377,
     "status": "ok",
     "timestamp": 1657212039899,
     "user": {
      "displayName": "Filippo Uslenghi",
      "userId": "02060675236977838476"
     },
     "user_tz": -120
    },
    "id": "YdZe2-WENlAe",
    "outputId": "fdad4070-2f5c-4c50-e247-009b8801483a",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 18:55:03.755200: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-07-19 18:55:03.755345: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "Corrupt JPEG data: 252 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98/250 [==========>...................] - ETA: 6s - loss: 0.6810 - accuracy: 0.5915"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 128 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/250 [=============>................] - ETA: 5s - loss: 0.6718 - accuracy: 0.6027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 228 extraneous bytes before marker 0xd9\n",
      "Warning: unknown JFIF revision number 0.00\n",
      "Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/250 [======================>.......] - ETA: 2s - loss: 0.6396 - accuracy: 0.6337"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 396 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/250 [=========================>....] - ETA: 1s - loss: 0.6328 - accuracy: 0.6417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 0s - loss: 0.6232 - accuracy: 0.6514"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 18:55:15.154122: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 13s 49ms/step - loss: 0.6232 - accuracy: 0.6514 - val_loss: 0.5358 - val_accuracy: 0.7252\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.5120 - accuracy: 0.7449 - val_loss: 0.4865 - val_accuracy: 0.7614\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 11s 45ms/step - loss: 0.4367 - accuracy: 0.7946 - val_loss: 0.4360 - val_accuracy: 0.7899\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.3776 - accuracy: 0.8308 - val_loss: 0.4512 - val_accuracy: 0.7904\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.3226 - accuracy: 0.8569 - val_loss: 0.4304 - val_accuracy: 0.8095\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.2655 - accuracy: 0.8880 - val_loss: 0.4512 - val_accuracy: 0.8077\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.1998 - accuracy: 0.9196 - val_loss: 0.4912 - val_accuracy: 0.8077\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.1399 - accuracy: 0.9462 - val_loss: 0.5013 - val_accuracy: 0.8102\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.0852 - accuracy: 0.9702 - val_loss: 0.6567 - val_accuracy: 0.8065\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0574 - accuracy: 0.9799 - val_loss: 0.8489 - val_accuracy: 0.7872\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.0507 - accuracy: 0.9835 - val_loss: 0.7676 - val_accuracy: 0.8162\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0290 - accuracy: 0.9907 - val_loss: 0.8958 - val_accuracy: 0.8075\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0332 - accuracy: 0.9884 - val_loss: 0.9311 - val_accuracy: 0.8077\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.9085 - val_accuracy: 0.8180\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0252 - accuracy: 0.9921 - val_loss: 0.8512 - val_accuracy: 0.8132\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.0291 - accuracy: 0.9905 - val_loss: 0.9134 - val_accuracy: 0.8032\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.0188 - accuracy: 0.9943 - val_loss: 1.0262 - val_accuracy: 0.8097\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 1.0549 - val_accuracy: 0.8102\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0185 - accuracy: 0.9933 - val_loss: 1.1041 - val_accuracy: 0.8100\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0159 - accuracy: 0.9945 - val_loss: 1.2079 - val_accuracy: 0.8140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1684c9f70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "model.fit(\n",
    "    train,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def zero_one_loss(dataset, dataset_size):\n",
    "    \n",
    "    _, accuracy = model.evaluate(dataset)\n",
    "    zero_one_loss = dataset_size*(1-accuracy)\n",
    "\n",
    "    return int(round(zero_one_loss, ndigits=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 20ms/step - loss: 1.1761 - accuracy: 0.8146\n",
      "Zero-one loss on the test set: 927\n"
     ]
    }
   ],
   "source": [
    "print(f\"Zero-one loss on the test set: {zero_one_loss(test, test_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any previous state\n",
    "# del train, test, model, dataset, test_size, formats, non_jpegs\n",
    "del model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "69XJrMJsiR03",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import KFold\\n\\n\\nk_fold = KFold(n_splits=5, shuffle=True)\\nk_splits = k_fold.split(data_paths)\\nresults = []\\n\\nfor train_index, test_index in k_splits:\\n\\n    # Get the paths to the data\\n    train_paths = np.asarray(data_paths)[train_index]\\n    test_paths = np.asarray(data_paths)[test_index]\\n\\n    # Make it tf.data.Dataset\\n    train = tf.data.Dataset.from_tensor_slices(train_paths)\\n    test = tf.data.Dataset.from_tensor_slices(test_paths)\\n\\n    # Shuffle the dataset\\n    train = train.shuffle(len(train))\\n    test = test.shuffle(len(test))\\n\\n    # Get labels\\n    train = train.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\\n    test = test.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\\n    \\n    # Configure for performance\\n    train = configure_for_performance(train)\\n    test = configure_for_performance(test)\\n\\n    # Create the model\\n    model = tf.keras.Sequential([\\n        tf.keras.layers.Rescaling(1./255),\\n        tf.keras.layers.Conv2D(16, 3, activation=\\'relu\\', input_shape=(150, 150, 3)),\\n        tf.keras.layers.MaxPooling2D(),\\n        tf.keras.layers.Dropout(0.1),\\n        tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\'),\\n        tf.keras.layers.MaxPooling2D(),\\n        tf.keras.layers.Dropout(0.2),\\n        tf.keras.layers.Conv2D(64, 3, activation=\\'relu\\'),\\n        tf.keras.layers.MaxPooling2D(),\\n        tf.keras.layers.Flatten(),\\n        tf.keras.layers.Dropout(0.3),\\n        tf.keras.layers.Dense(128, activation=\\'relu\\'),\\n        tf.keras.layers.Dropout(0.4),\\n        tf.keras.layers.Dense(1, activation=\\'sigmoid\\')\\n    ])\\n\\n    model.compile(\\n        optimizer=\\'adam\\',\\n        loss=\\'binary_crossentropy\\',\\n        metrics=[\\'accuracy\\']\\n    )\\n\\n    model.fit(\\n        train,\\n        epochs=n_epochs,\\n        verbose=0,\\n    )\\n\\n    loss = zero_one_loss(test, len(test_paths))\\n    results.append(loss)\\n    print(f\"Zero-one loss: {loss}\")\\n\\n    # Clear any previous state\\n    del model\\n    tf.keras.backend.clear_session()\\n    gc.collect()\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "k_splits = k_fold.split(data_paths)\n",
    "results = []\n",
    "\n",
    "for train_index, test_index in k_splits:\n",
    "\n",
    "    # Get the paths to the data\n",
    "    train_paths = np.asarray(data_paths)[train_index]\n",
    "    test_paths = np.asarray(data_paths)[test_index]\n",
    "\n",
    "    # Make it tf.data.Dataset\n",
    "    train = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "    test = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    train = train.shuffle(len(train))\n",
    "    test = test.shuffle(len(test))\n",
    "\n",
    "    # Get labels\n",
    "    train = train.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test = test.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Configure for performance\n",
    "    train = configure_for_performance(train)\n",
    "    test = configure_for_performance(test)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train,\n",
    "        epochs=n_epochs,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    loss = zero_one_loss(test, len(test_paths))\n",
    "    results.append(loss)\n",
    "    print(f\"Zero-one loss: {loss}\")\n",
    "\n",
    "    # Clear any previous state\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Z5LsUvuXDG9g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nmean_loss = np.round(np.mean(results), decimals=0)\\nstd_loss = np.round(np.std(results), decimals=0)\\nprint(f'The mean of zero-one loss is {int(mean_loss)}, with a standard deviation of {int(std_loss)} missmatched samples')\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "mean_loss = np.round(np.mean(results), decimals=0)\n",
    "std_loss = np.round(np.std(results), decimals=0)\n",
    "print(f'The mean of zero-one loss is {int(mean_loss)}, with a standard deviation of {int(std_loss)} missmatched samples')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bb77c6cb71ba4a7f7f0e8e08f8700abef1980870bd72f32fe1d9c2ef1e40dce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
